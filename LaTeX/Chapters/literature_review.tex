\section{Past Approaches of Solving Optimal Trade Execution}
Several approaches have been taken in the past to tackle the optimal trading execution problem. \cite{A1} developed a theoretical model for optimal trade execution based on maximizing expected utility at different levels of risk aversion. \cite{A2} used a stochastic recursive procedure to calculate the optimal distance from the bid-ask spread to post passive limit orders. Studies have also been conducted using machine learning where models are back tested on historical data. \cite{A5} used genetic algorithms to develop an optimal trading strategy using Australian Stock Exchange data. \cite{A3} used reinforcement learning to solve the problem for several stocks listed on the NASDAQ. Its methodology is summarized below:


Once again, the goal of the agent (the institution that is attempting to optimally trade) was to buy $V$ shares in a time horizon $T$. First, the time horizon was divided into discrete time steps, where any number of shares could be bought at each step. Reinforcement learning is a state-based strategy, meaning that the agent examines the state of the Markovian system to decide what to do next. The state of the system is modelled by two private variables that represent the number of shares left to buy and the amount of time left, as well as several market variables that represent various properties of the LOB. At each time step, an action could be taken, which was to place a market or limit order at a specified volume. At the end, all remaining shares had to be bought/sold in a market order.  After each time step, a reward is then received, which is the cash inflow if selling or negative outflow if buying. In each state, every possible action is attempted and one is chosen that maximizes expected reward. The system then transitions to the next state and updates the expected reward function. The study based awards and market variables on historical high-frequency NASDAQ data, and assumed that the actions of the agent did not affect future dynamics of the order book. 


This thesis seeks to evaluate trading strategies that attempt to solve optimal trade execution problem, but with the impact of the agent’s actions taken into account. In order to accomplish this, we must first simulate the LOB while updating its dynamics when a trade occurs. To do so, we simulate the LOB using a modified version of the queue reactive model of the LOB by \cite{A6}, which is described in Section~\ref{modelLOB}.

\section{Queue Reactive Model} \label{modelLOB}
In this section, we describe the queue reactive model used to simulate the LOB that is based on the one developed by \cite{A6}. The LOB is modelled as a $2K$-dimensional vector $Q$, where $K$ is number of limits on each side of the Markovian queuing system. Let $p_{ref}$ be the reference price of the of the stock. Then, $[Q_k: k = -1 \ldots -K]$ represents the number of orders on the bid side $k - 0.5$ ticks to the left of $p_{ref}$ and $[Q_k: k =1 \ldots K]$ represents the number of orders on the ask side $k - 0.5$ ticks to the right of $p_{ref}$. Then, $X(t) = (q_{-K}(t), … q_{-1}(t), q_1(t), … , q_K(t))$ is a continuous-time Markov jump process with a jump size equal to one.


An event at position $k$ occurs any time that the number of shares offered at position $k$ changes. It increases when a trader places a limit order at the price of position $i$ and decreases when it is either consumed by a market order from the other side or the trader cancels the limit order.


The simplest model assumes that $Q_k$ is independent for each $k$. Each jump when $Q_k$ increases by 1 and $Q = q$ follows a Poisson arrival with rate $\lambda^+_k(q)$ and each jump when $Q_k$ decreases by 1 and $Q = q$ follows a Poisson arrival with rate $\lambda^-_k(q)$. In order to model each $Q_k$ as an integer with changes equal to 1, we use $Q_k$ to represent the number of Average Event Sizes ($AES_k$) present at the LOB position. $AES$ is the average size of the absolute change in the number of shares at the queue position whenever a change occurs, where each $AES_k$ could differ. In this model, $Q_k = n$ would mean that there are $n*AES_k$ shares offered at position $k$.


We choose $p_{ref}$ to be a price in between 2 adjacent ticks, so that an events at each position occur at actual ticks. Let b be the best bid price and a be the best ask price. If $b$ and $a$ are an odd number of ticks apart, then we have 
$$p_{ref}=  (b+a)/2$$
If $b$ and $a$ are an even number of ticks apart, then we could pick $p_{ref}$ to be $(b+a)/2 + 0.5$ or $(b+a)/2 - 0.5$. We choose the one that is closest to the previous reference price.

If an event occurs that changes the best bid or ask, then we update $p_{ref}$. It can be seen that under certain conditions, this model is an ergodic Markov process (see \cite{A6}). We can then use historical LOB data to estimate $AES_k$, $\lambda^+_k(q)$, and $\lambda^-_k(q)$ for each $k$.

\section{Market Simulation}
Studies in the past have also used models similar to the queue reactive model by modelling arrivals as Poisson processes. For example, \cite{A9} modelled price jumps of an asset using a bi-dimensional Hawkes process. Simulation also includes correlated Poisson processes, whose construction uses copulas as described by \cite{A8} and backwards simulation as described by \cite{A7}. 