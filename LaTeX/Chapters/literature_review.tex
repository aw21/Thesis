\section{Past Approaches of Solving Optimal Trade Execution}
Several approaches have been taken in the past to tackle the optimal trading execution problem. \cite{A1} developed a theoretical model for optimal trade execution based on maximizing expected utility at different levels of risk aversion. \cite{A2} used a stochastic recursive procedure to calculate the optimal distance from the bid-ask spread to post passive limit orders. Studies have also been conducted using machine learning where models are back tested on historical data. \cite{A5} used genetic algorithms to develop an optimal trading strategy using Australian Stock Exchange data. \cite{A3} used reinforcement learning to solve the problem for several stocks listed on the NASDAQ. This thesis hopes to expand on this paper, whose methodology is briefly summarized below.


Once again, the goal of the agent (the institution that is attempting to optimally trade) was to buy $V$ shares in a time horizon $T$. First, the time horizon was divided into discrete time steps, where any number of shares could be bought at each step. Reinforcement learning is a state-based strategy, meaning that the agent examines the state of the Markovian system to decide what to do next. The state of the system is modelled by two private variables that represent the number of shares left to buy and the amount of time left, as well as several market variables that represent various properties of the LOB. At each time step, an action could be taken, which was to place a market or limit order at a specified volume. At the end, all remaining shares had to be bought/sold in a market order.  After each time step, a reward is then received, which is the cash inflow if selling or negative outflow if buying. In each state, every possible action is attempted and one is chosen that maximizes expected reward. The system then transitions to the next state and updates the expected reward function. The study based awards and market variables on historical high-frequency NASDAQ data, and assumed that the actions of the agent did not affect future dynamics of the order book. 


Although studies using reinforcement learning on LOBs have been conducted in the past (\cite{A3}, \cite{A4}), neither included the impact of the agent’s trades on the future dynamics of the LOB. This thesis seeks to use reinforcement learning to solve the optimal trade execution problem, but with the impact of the agent’s actions taken into account. In order to accomplish this, we must first simulate the LOB while updating its dynamics when a trade occurs. To do so, we simulate the LOB based on queue reactive model of the LOB by \cite{A6}, which is described in Section~\ref{modelLOB}. We can then build the reinforcement learning model within this simulated environment.

\section{Queue Reactive Model} \label{modelLOB}
In this section, we describe the queue reactive model used to simulate the LOB that is based on the one developed by \cite{A6}. The LOB is modelled as a $2K$-dimensional vector $Q$, where $K$ is number of limits on each side of the Markovian queuing system. Let $p_{ref}$ be the reference price of the of the stock. Then, $[Qi: i = -1 \ldots -K]$ represents the number of orders on the bid side $i - 0.5$ ticks to the left of $p_{ref}$ and $[Qi: i =1 \ldots K]$ represents the number of orders on the ask side $i - 0.5$ ticks to the right of $p_{ref}$. Then, $X(t) = (q_{-K}(t), … q_{-1}(t), q_1(t), … , q_K(t))$ is a continuous-time Markov jump process with a jump size equal to one.


An event at position $i$ occurs any time that the number of shares offered at position $i$ changes. It increases when a trader places a limit order at the price of position $i$ and decreases when it is either consumed by a market order from the other side or the trader cancels the limit order.


For simplicity, we for now assume that $Q_i$ is independent for each $i$. Each jump when $Q_i$ increases by 1 and $Q = q$ follows a Poisson arrival with rate $\lambda_i(q)$ and each jump when $Q_i$ decreases by 1 and $Q = q$ follows a Poisson arrival with rate $\gamma_i(q)$. For now, we might assume that the arrival rates at each $i$ are independent of the size of the queue at any other $i$, but we may explore more complicated formulations in the course of the thesis. In order to model each $Q_i$ as an integer with changes equal to 1, we use $Q_i$ to represent the number of Average Event Sizes ($AES_i$) present at the LOB position. $AES$ is the average size of the absolute change in the number of shares at the queue position whenever a change occurs, where each $AES_i$ could differ. In this model, $Q_i = n$ would mean that there is $n*AESi$ shares offered at position $i$.


We choose $p_{ref}$ to be a price in between 2 adjacent ticks, so that an events at each position occur at actual ticks. Let b be the best bid price and a be the best ask price. If $b$ and $a$ are an odd number of ticks apart, then we have 
$$p=  (b+a)/2$$
If $b$ and $a$ are an even number of ticks apart, then we have
$$p=  (b+a)/2+0.5$$

Note that picking the reference price to be closer to the ask side is arbitrary. We have chosen this now for simplicity, but may revise it later to something more complicated (perhaps the side closest to the last reference price). If an event occurs that changes the best bid or ask, then we update $p_{ref}$. It can be seen that under certain conditions, this model is an ergodic Markov process (see \cite{A6}). We can then use historical LOB data to estimate $AES_i$, $\lambda_i(q)$, and $\gamma_i(q)$ for each $i$.