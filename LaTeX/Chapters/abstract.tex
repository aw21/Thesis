The electronic nature of markets today has allowed traders to infer information from limit order book (LOB) data and execute trades based on this information. In this paper, we attempt to solve the problem of optimal trade execution using reinforcement learning in a simulated market. Given the current state of the limit order book for a particular asset and the amount to buy (or sell), we model the problem as a Markov decision process and output the orders to place at each time step. Reinforcement learning has recently been used to optimize various agents' decisions given LOB data. However, it has relied on historical LOB data, so it has not accounted for the market impact on LOB dynamics from previous actions. To my knowledge, this paper is the first attempt of its kind to use reinforcement learning while updating LOB dynamics in real time. We first use high frequency LOB data to simulate LOB dynamics with real-time market impact adjustments using the queue reactive model developed by Huang et al. (2013). We then use reinforcement learning in the vein of Nevmyvaka et al. (2006) to model optimal trade decisions at each time period. We also explore various techniques to optimize performance of our model. Finally, we evaluate the performance of our model on testing data and compare it to common trade execution strategies. The findings from this paper are relevant for institutions who want to execute trades while maintaining profit as well as those who want to evaluate the effectiveness of reinforcement learning for trading decisions.